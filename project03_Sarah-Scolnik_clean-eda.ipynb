{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import comment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Your skin is flawless. Maybe you can bottle Fi...</td>\n",
       "      <td>cats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Such a beautiful cat ^^</td>\n",
       "      <td>cats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is someone's OC</td>\n",
       "      <td>cats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It totally makes my day to meet a random cat o...</td>\n",
       "      <td>cats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Looks just like my kitty Blue! He does the sam...</td>\n",
       "      <td>cats</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body subreddit\n",
       "0  Your skin is flawless. Maybe you can bottle Fi...      cats\n",
       "1                            Such a beautiful cat ^^      cats\n",
       "2                               this is someone's OC      cats\n",
       "3  It totally makes my day to meet a random cat o...      cats\n",
       "4  Looks just like my kitty Blue! He does the sam...      cats"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Your skin is flawless. Maybe you can bottle Fi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Such a beautiful cat ^^</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is someone's OC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It totally makes my day to meet a random cat o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Looks just like my kitty Blue! He does the sam...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  target\n",
       "0  Your skin is flawless. Maybe you can bottle Fi...       1\n",
       "1                            Such a beautiful cat ^^       1\n",
       "2                               this is someone's OC       1\n",
       "3  It totally makes my day to meet a random cat o...       1\n",
       "4  Looks just like my kitty Blue! He does the sam...       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target is 'subreddit'. Current values: 'cats', 'dogs'.\n",
    "# Need to transform to 0 or 1 values. Set cats = 1, dogs = 0\n",
    "\n",
    "df['target'] = df['subreddit'].map({'cats': 1, 'dogs': 0})\n",
    "df.drop('subreddit', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***to do***\n",
    "# maybe: remove the duplicate mod comments, [removed] comments, etc but keep repetitive comments i.e. Thank you!\n",
    "# remove all of the RemindMe! posts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1649"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are some duplicate comments, mostly automated comments by moderators\n",
    "\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>PLEASE READ THE ENTIRE MESSAGE BEFORE MESSAGIN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>PLEASE READ THE ENTIRE MESSAGE BEFORE MESSAGIN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>PLEASE READ THE ENTIRE MESSAGE BEFORE MESSAGIN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>PLEASE READ THE ENTIRE MESSAGE BEFORE MESSAGIN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>PLEASE READ THE ENTIRE MESSAGE BEFORE MESSAGIN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 body  target\n",
       "26  PLEASE READ THE ENTIRE MESSAGE BEFORE MESSAGIN...       1\n",
       "45  PLEASE READ THE ENTIRE MESSAGE BEFORE MESSAGIN...       1\n",
       "47  PLEASE READ THE ENTIRE MESSAGE BEFORE MESSAGIN...       1\n",
       "50  PLEASE READ THE ENTIRE MESSAGE BEFORE MESSAGIN...       1\n",
       "60  PLEASE READ THE ENTIRE MESSAGE BEFORE MESSAGIN...       1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19962</th>\n",
       "      <td>Your post has been automatically removed becau...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19985</th>\n",
       "      <td>Your post has been automatically removed becau...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19986</th>\n",
       "      <td>Your post has been automatically removed becau...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19989</th>\n",
       "      <td>Your post has been automatically removed becau...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>Your post has been automatically removed becau...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    body  target\n",
       "19962  Your post has been automatically removed becau...       0\n",
       "19985  Your post has been automatically removed becau...       0\n",
       "19986  Your post has been automatically removed becau...       0\n",
       "19989  Your post has been automatically removed becau...       0\n",
       "19996  Your post has been automatically removed becau...       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated()].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"PLEASE READ THE ENTIRE MESSAGE BEFORE MESSAGING THE MODTEAM.\\nBecause your account is new (under 10 days old) **OR your account has very low comment karma**, your submission has been removed.\\nThis action is **NOT** directed at you personally; /r/cats requires all accounts to be at least **10 days old** AND have **at least 25 comment karma** in order to create new threads.\\nComment karma is **not** the same thing as link karma. If you have less than 25 comment karma, it is easy to get- simply participate in a few discussions and you'll have the amount you need in no time at all.\\nQuestions? [Message the mods](http://www.reddit.com/message/compose?to=%2Fr%2Fcats) and let us know.\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cats) if you have any questions or concerns.*\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catmodpost = list(df[df.duplicated()]['body'])[0]\n",
    "catmodpost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of duplicate cat mod posts\n",
    "len(df[df['body'] == catmodpost])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your post has been automatically removed because you did not include one of the required title tags. Please see the [subreddit rules](https://www.reddit.com/r/dogs/wiki/index#wiki_tags_and_descriptions) for more information.  Potential title tags include: [Breeds], [Help], [Vent], [RIP], [Fluff], [Discussion], [Link], [Meta], [Survey], and [Update].  Please resubmit your post with one of the title tags beginning the submission title.  You must physically type the tag into the title, and the tag must use square brackets.  Example: \"[Discussion] What foods are toxic to dogs?\"  It\\'s possible you may experience a delay of up to 1 hour upon trying to repost with the corrected title.  This is a Reddit-imposed waiting period and there is nothing the /r/dogs moderators can do to reduce the waiting period.  We apologize for the inconvenience.\\n\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dogs) if you have any questions or concerns.*'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dogmodpost = list(df[df.duplicated()]['body'])[-1]\n",
    "dogmodpost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "630"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of duplicate dog mod posts\n",
    "len(df[df['body'] == dogmodpost])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>😂</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>Thank you!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>❤️</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>❤️</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>I’m kinda envious, finding a good vet here in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>This post was removed as it matched the keywor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>[removed]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>❤️🌈❤️</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>❤️🌈❤️</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>Gorgeous!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body  target\n",
       "241                                                   😂       1\n",
       "501                                          Thank you!       1\n",
       "698                                                  ❤️       1\n",
       "699                                                  ❤️       1\n",
       "795   I’m kinda envious, finding a good vet here in ...       1\n",
       "825   This post was removed as it matched the keywor...       1\n",
       "960                                           [removed]       1\n",
       "1079                                              ❤️🌈❤️       1\n",
       "1105                                              ❤️🌈❤️       1\n",
       "1106                                         Gorgeous!        1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at other duplicate posts\n",
    "df[df.duplicated() & (df['body'] != catmodpost) & (df['body'] != dogmodpost)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19417</th>\n",
       "      <td>RemindMe! 18 hours</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19432</th>\n",
       "      <td>Following</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19441</th>\n",
       "      <td>RemindMe! 18 hours</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19451</th>\n",
       "      <td>RemindMe! 18 hours</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19466</th>\n",
       "      <td>Remind me! 2 days</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19664</th>\n",
       "      <td>I would add up to a minimum of one hour per da...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19706</th>\n",
       "      <td>Can I ask how expensive the Cytopoint is for y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19844</th>\n",
       "      <td>It seems like you may be asking about breeds t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19901</th>\n",
       "      <td>Thank you</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19951</th>\n",
       "      <td>It seems like you may be asking about breeds t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    body  target\n",
       "19417                                 RemindMe! 18 hours       0\n",
       "19432                                          Following       0\n",
       "19441                                 RemindMe! 18 hours       0\n",
       "19451                                 RemindMe! 18 hours       0\n",
       "19466                                  Remind me! 2 days       0\n",
       "19664  I would add up to a minimum of one hour per da...       0\n",
       "19706  Can I ask how expensive the Cytopoint is for y...       0\n",
       "19844  It seems like you may be asking about breeds t...       0\n",
       "19901                                         Thank you        0\n",
       "19951  It seems like you may be asking about breeds t...       0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at other duplicate posts\n",
    "df[df.duplicated() & (df['body'] != catmodpost) & (df['body'] != dogmodpost)].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18351, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19994</th>\n",
       "      <td>I'd cancel. I had a dog live just short of 18 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>Well, if she didn't have ANY KIND OF IDENTIFIC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>This is a little late, but here goes:\\n\\nOur d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>Lots of cuteness, over time.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>I’ve got one that’s embroidered now, but none ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    body  target\n",
       "19994  I'd cancel. I had a dog live just short of 18 ...       0\n",
       "19995  Well, if she didn't have ANY KIND OF IDENTIFIC...       0\n",
       "19997  This is a little late, but here goes:\\n\\nOur d...       0\n",
       "19998                       Lots of cuteness, over time.       0\n",
       "19999  I’ve got one that’s embroidered now, but none ...       0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18346</th>\n",
       "      <td>I'd cancel. I had a dog live just short of 18 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18347</th>\n",
       "      <td>Well, if she didn't have ANY KIND OF IDENTIFIC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18348</th>\n",
       "      <td>This is a little late, but here goes:\\n\\nOur d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18349</th>\n",
       "      <td>Lots of cuteness, over time.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18350</th>\n",
       "      <td>I’ve got one that’s embroidered now, but none ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    body  target\n",
       "18346  I'd cancel. I had a dog live just short of 18 ...       0\n",
       "18347  Well, if she didn't have ANY KIND OF IDENTIFIC...       0\n",
       "18348  This is a little late, but here goes:\\n\\nOur d...       0\n",
       "18349                       Lots of cuteness, over time.       0\n",
       "18350  I’ve got one that’s embroidered now, but none ...       0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used these links as a reference: \n",
    "# https://towardsdatascience.com/the-real-world-as-seen-on-twitter-sentiment-analysis-part-one-5ac2d06b63fb\n",
    "# https://stackoverflow.com/questions/4328500/how-can-i-strip-all-punctuation-from-a-string-in-javascript-using-regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***to do***\n",
    "# remove numbers and words that start w/numbers i.e. \n",
    "#'3am', '3ish', '3keywords', '3lb', '3oz', '3rd', '3yo', '3yrs', '400', '4000', '40701psc', '40a37711', '40ish', '40lbs', '40pounds', '41kg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of these are redundant with the default functions of CountVectorizer but that's OK\n",
    "\n",
    "def cleaner(text):\n",
    "    # Make lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove HTML special entities (e.g. &amp;)\n",
    "    text = re.sub(r'\\&\\w*;', '', text)\n",
    "    \n",
    "    # Remove hyperlinks\n",
    "    text = re.sub(r'https?:\\/\\/.*\\/\\w*', '', text)\n",
    "    \n",
    "    # Remove punctuation and split 's, 't, 've with a space for filter\n",
    "    text = re.sub(r'[' + string.punctuation.replace('@', '') + ']+', ' ', text)\n",
    "    \n",
    "    # Remove words with 2 or fewer letters\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
    "    \n",
    "    # Remove whitespace (including new line characters)\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)\n",
    "    \n",
    "    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n",
    "    text = ''.join(c for c in text if c <= '\\uFFFF') \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body'] = df['body'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18351, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop rows where body = ''\n",
    "df = df[df['body'] != '']\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18270, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP pre-processing and exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    words = text.split()\n",
    "    lemma_words = ''\n",
    "    for word in words:\n",
    "        lemma_words += (lemmatizer.lemmatize(word) + ' ')\n",
    "    return lemma_words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body'] = df['body'].apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18270, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop rows where body = ''\n",
    "df = df[df['body'] != '']\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18228, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('comments_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### most frequent cat words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy way to get most frequently used words: change max_features\n",
    "\n",
    "count_vect = CountVectorizer(analyzer = \"word\", \n",
    "                             tokenizer = None, \n",
    "                             preprocessor = None,\n",
    "                             stop_words = \"english\", \n",
    "                             max_features = 35) \n",
    "\n",
    "# input for CountVectorizer is an array of strings\n",
    "vector_input_cats = df[df['target'] == 1]['body']\n",
    "\n",
    "# fit_transform the vectorizer\n",
    "cat_words = count_vect.fit_transform(vector_input_cats)\n",
    "\n",
    "# convert output to a Numpy array\n",
    "cat_words = cat_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beautiful', 'best', 'cat', 'cute', 'day', 'doe', 'don', 'food', 'good', 'got', 'ha', 'home', 'just', 'kitten', 'kitty', 'know', 'life', 'like', 'little', 'lol', 'look', 'love', 'make', 'old', 'really', 'sorry', 'sure', 'thank', 'thing', 'think', 'time', 'vet', 'wa', 'want', 'year']\n"
     ]
    }
   ],
   "source": [
    "# get the words\n",
    "cat_word_list = count_vect.get_feature_names()\n",
    "print(cat_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### most frequent dog words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy way to get most frequently used words: change max_features\n",
    "\n",
    "count_vect = CountVectorizer(analyzer = \"word\", \n",
    "                             tokenizer = None, \n",
    "                             preprocessor = None,\n",
    "                             stop_words = \"english\", \n",
    "                             max_features = 36) \n",
    "\n",
    "# input for CountVectorizer is an array of strings\n",
    "vector_input_dogs = df[df['target'] == 0]['body']\n",
    "\n",
    "# fit_transform the vectorizer\n",
    "dog_words = count_vect.fit_transform(vector_input_dogs)\n",
    "\n",
    "# convert output to a Numpy array\n",
    "dog_words = dog_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['breed', 'breeder', 'day', 'doesn', 'dog', 'don', 'food', 'going', 'good', 'got', 'ha', 'help', 'home', 'just', 'know', 'like', 'look', 'lot', 'love', 'make', 'need', 'people', 'pet', 'puppy', 'really', 'sure', 'thing', 'think', 'time', 'training', 'vet', 'wa', 'want', 'way', 'work', 'year']\n"
     ]
    }
   ],
   "source": [
    "# get the words\n",
    "dog_word_list = count_vect.get_feature_names()\n",
    "print(dog_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### edit stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add non-meaningful words from the \"most frequent\" lists above to the stop words dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'did',\n",
       "           'do',\n",
       "           'doe',\n",
       "           'doesn',\n",
       "           'don',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'getting',\n",
       "           'give',\n",
       "           'go',\n",
       "           'going',\n",
       "           'got',\n",
       "           'ha',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'isn',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'wa',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "text.ENGLISH_STOP_WORDS\n",
    "\n",
    "add_stop_words = ['did', 'doe', 'don', 'doesn', 'getting', 'going', 'got', 'ha', 'isn', 'wa']\n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cat vs dog top 100 words: []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***to do***\n",
    "# out of 100, how many are same & how many different?\n",
    "# make word clouds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word/n-gram frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "# CountVectorizer transforms the body text from the reddit comments into features (i.e. words)\n",
    "# and creates columns (vectors) with word counts for each comment\n",
    "\n",
    "count_vect = CountVectorizer(analyzer = \"word\", \n",
    "                             tokenizer = None, \n",
    "                             preprocessor = None,\n",
    "                             stop_words = stop_words, \n",
    "                             max_features = 10000, \n",
    "                             ngram_range=(1, 3)\n",
    "                            ) \n",
    "\n",
    "# input for CountVectorizer is an array of strings\n",
    "vector_input_cats = df[df['target'] == 1]['body']\n",
    "\n",
    "# fit_transform the vectorizer\n",
    "cat_words = count_vect.fit_transform(vector_input_cats)\n",
    "\n",
    "# convert output to a Numpy array\n",
    "cat_words = cat_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat          3030\n",
       "like         1026\n",
       "just          843\n",
       "love          680\n",
       "kitty         601\n",
       "look          587\n",
       "time          500\n",
       "good          407\n",
       "little        406\n",
       "know          384\n",
       "vet           377\n",
       "day           367\n",
       "food          366\n",
       "thank         365\n",
       "sorry         349\n",
       "year          346\n",
       "cute          342\n",
       "beautiful     340\n",
       "thing         333\n",
       "think         319\n",
       "kitten        315\n",
       "make          297\n",
       "really        292\n",
       "want          273\n",
       "sure          258\n",
       "lol           251\n",
       "home          247\n",
       "life          244\n",
       "best          233\n",
       "old           221\n",
       "right         214\n",
       "look like     214\n",
       "lot           213\n",
       "need          212\n",
       "eye           206\n",
       "pet           202\n",
       "baby          198\n",
       "help          196\n",
       "hope          195\n",
       "happy         193\n",
       "way           190\n",
       "say           188\n",
       "loss          184\n",
       "great         184\n",
       "try           182\n",
       "boy           177\n",
       "long          174\n",
       "thanks        174\n",
       "pretty        172\n",
       "sweet         169\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ***to do*** (optional)\n",
    "# if the array is very large, it's faster to do a sum of the array first (which creates a vector of the sums)\n",
    "# and then combine that with the feature names in a dataframe\n",
    "\n",
    "cat_matrix = pd.DataFrame(cat_words, columns=count_vect.get_feature_names())\n",
    "\n",
    "cat_matrix.sum().sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat          0.335362\n",
       "like         0.113558\n",
       "just         0.093304\n",
       "love         0.075263\n",
       "kitty        0.066519\n",
       "look         0.064970\n",
       "time         0.055340\n",
       "good         0.045047\n",
       "little       0.044936\n",
       "know         0.042501\n",
       "vet          0.041727\n",
       "day          0.040620\n",
       "food         0.040509\n",
       "thank        0.040398\n",
       "sorry        0.038628\n",
       "year         0.038296\n",
       "cute         0.037853\n",
       "beautiful    0.037631\n",
       "thing        0.036857\n",
       "think        0.035307\n",
       "kitten       0.034864\n",
       "make         0.032872\n",
       "really       0.032319\n",
       "want         0.030216\n",
       "sure         0.028556\n",
       "lol          0.027781\n",
       "home         0.027338\n",
       "life         0.027006\n",
       "best         0.025789\n",
       "old          0.024460\n",
       "right        0.023686\n",
       "look like    0.023686\n",
       "lot          0.023575\n",
       "need         0.023464\n",
       "eye          0.022800\n",
       "pet          0.022357\n",
       "baby         0.021915\n",
       "help         0.021693\n",
       "hope         0.021583\n",
       "happy        0.021361\n",
       "way          0.021029\n",
       "say          0.020808\n",
       "loss         0.020365\n",
       "great        0.020365\n",
       "try          0.020144\n",
       "boy          0.019590\n",
       "long         0.019258\n",
       "thanks       0.019258\n",
       "pretty       0.019037\n",
       "sweet        0.018705\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_matrix.mean().sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer = \"word\", \n",
    "                             tokenizer = None, \n",
    "                             preprocessor = None,\n",
    "                             stop_words = stop_words, \n",
    "                             max_features = 10000, \n",
    "                             ngram_range=(1, 3)\n",
    "                            ) \n",
    "\n",
    "# input for CountVectorizer is an array of strings\n",
    "vector_input_dogs = df[df['target'] == 0]['body']\n",
    "\n",
    "# fit_transform the vectorizer\n",
    "dog_words = count_vect.fit_transform(vector_input_dogs)\n",
    "\n",
    "# convert output to a Numpy array\n",
    "dog_words = dog_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dog         10783\n",
       "just         2612\n",
       "like         2481\n",
       "time         1857\n",
       "know         1462\n",
       "people       1369\n",
       "think        1304\n",
       "good         1279\n",
       "puppy        1255\n",
       "day          1188\n",
       "thing        1145\n",
       "really       1126\n",
       "breed        1106\n",
       "make         1081\n",
       "vet          1048\n",
       "want         1044\n",
       "need         1011\n",
       "work          919\n",
       "lot           851\n",
       "year          846\n",
       "food          843\n",
       "love          829\n",
       "way           822\n",
       "home          785\n",
       "breeder       690\n",
       "training      688\n",
       "sure          668\n",
       "look          665\n",
       "help          652\n",
       "pet           648\n",
       "life          644\n",
       "say           636\n",
       "right         616\n",
       "walk          594\n",
       "try           587\n",
       "come          577\n",
       "animal        570\n",
       "issue         569\n",
       "best          565\n",
       "owner         561\n",
       "great         559\n",
       "month         544\n",
       "long          537\n",
       "old           532\n",
       "let           525\n",
       "little        520\n",
       "feel          511\n",
       "didn          506\n",
       "week          502\n",
       "shelter       501\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_matrix = pd.DataFrame(dog_words, columns=count_vect.get_feature_names())\n",
    "\n",
    "dog_matrix.sum().sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dog         1.172958\n",
       "just        0.284129\n",
       "like        0.269879\n",
       "time        0.202002\n",
       "know        0.159034\n",
       "people      0.148918\n",
       "think       0.141847\n",
       "good        0.139128\n",
       "puppy       0.136517\n",
       "day         0.129229\n",
       "thing       0.124551\n",
       "really      0.122484\n",
       "breed       0.120309\n",
       "make        0.117589\n",
       "vet         0.114000\n",
       "want        0.113565\n",
       "need        0.109975\n",
       "work        0.099967\n",
       "lot         0.092570\n",
       "year        0.092027\n",
       "food        0.091700\n",
       "love        0.090177\n",
       "way         0.089416\n",
       "home        0.085391\n",
       "breeder     0.075057\n",
       "training    0.074840\n",
       "sure        0.072664\n",
       "look        0.072338\n",
       "help        0.070924\n",
       "pet         0.070488\n",
       "life        0.070053\n",
       "say         0.069183\n",
       "right       0.067008\n",
       "walk        0.064614\n",
       "try         0.063853\n",
       "come        0.062765\n",
       "animal      0.062004\n",
       "issue       0.061895\n",
       "best        0.061460\n",
       "owner       0.061025\n",
       "great       0.060807\n",
       "month       0.059175\n",
       "long        0.058414\n",
       "old         0.057870\n",
       "let         0.057109\n",
       "little      0.056565\n",
       "feel        0.055586\n",
       "didn        0.055042\n",
       "week        0.054607\n",
       "shelter     0.054498\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_matrix.mean().sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat           323.546218\n",
       "like          150.042255\n",
       "love          148.794817\n",
       "look          126.013153\n",
       "cute          125.693852\n",
       "thank         117.198481\n",
       "beautiful     115.932920\n",
       "kitty         115.234998\n",
       "just          112.678251\n",
       "sorry          99.064591\n",
       "good           80.264965\n",
       "little         78.539472\n",
       "lol            72.069564\n",
       "know           71.078370\n",
       "time           70.687266\n",
       "adorable       70.011383\n",
       "eye            64.779981\n",
       "thanks         60.235855\n",
       "look like      58.835983\n",
       "think          56.964162\n",
       "want           55.660938\n",
       "pretty         54.594326\n",
       "sweet          54.539193\n",
       "right          54.296826\n",
       "best           53.828006\n",
       "loss           53.652515\n",
       "thing          53.228171\n",
       "kitten         51.525262\n",
       "sure           50.449468\n",
       "really         50.191682\n",
       "yes            49.957459\n",
       "day            49.916682\n",
       "sorry loss     49.396057\n",
       "make           49.178146\n",
       "great          48.878039\n",
       "year           48.332306\n",
       "baby           47.900873\n",
       "vet            47.825943\n",
       "food           47.238175\n",
       "gorgeous       46.372627\n",
       "happy          46.140763\n",
       "face           45.699652\n",
       "need           45.359183\n",
       "boy            44.873403\n",
       "hope           43.583604\n",
       "life           43.432378\n",
       "say            42.717325\n",
       "omg            41.702465\n",
       "handsome       41.569880\n",
       "pet            39.969835\n",
       "dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvec = TfidfVectorizer(analyzer = \"word\", \n",
    "                     stop_words = stop_words, \n",
    "                     max_features = 10000, \n",
    "                     ngram_range = (1, 3))\n",
    "\n",
    "cat_tf_words = tvec.fit_transform(vector_input_cats)\n",
    "\n",
    "cat_tf_words = cat_tf_words.toarray()\n",
    "\n",
    "cat_matrix = pd.DataFrame(cat_tf_words, columns=tvec.get_feature_names())\n",
    "\n",
    "cat_matrix.sum().sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dog           462.779419\n",
       "like          186.892332\n",
       "just          183.657466\n",
       "time          136.848902\n",
       "know          125.705360\n",
       "good          118.816490\n",
       "think         114.379939\n",
       "vet           111.545706\n",
       "thank         110.007991\n",
       "people        106.517715\n",
       "love          104.359127\n",
       "puppy         102.901687\n",
       "day            98.700575\n",
       "really         97.671905\n",
       "make           95.745516\n",
       "thing          94.670714\n",
       "breed          89.637384\n",
       "need           87.865439\n",
       "want           86.471091\n",
       "thanks         83.227666\n",
       "look           82.925065\n",
       "work           82.545906\n",
       "food           79.328188\n",
       "way            76.331721\n",
       "year           75.754686\n",
       "sure           75.416406\n",
       "lot            74.076969\n",
       "home           69.949664\n",
       "right          69.046597\n",
       "help           68.516200\n",
       "sound          68.157207\n",
       "try            66.881656\n",
       "life           65.164926\n",
       "say            64.271097\n",
       "best           63.432643\n",
       "great          62.749292\n",
       "little         62.700242\n",
       "pet            60.998215\n",
       "animal         60.556549\n",
       "walk           60.229329\n",
       "maybe          59.701631\n",
       "breeder        59.456961\n",
       "training       58.861043\n",
       "come           58.567829\n",
       "pup            58.496976\n",
       "sorry          57.377768\n",
       "definitely     57.238167\n",
       "old            56.605127\n",
       "issue          56.554245\n",
       "cat            56.317859\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvec = TfidfVectorizer(analyzer = \"word\", \n",
    "                     stop_words = stop_words, \n",
    "                     max_features = 10000, \n",
    "                     ngram_range = (1, 3))\n",
    "\n",
    "dog_tf_words = tvec.fit_transform(vector_input_dogs)\n",
    "\n",
    "dog_tf_words = dog_tf_words.toarray()\n",
    "\n",
    "dog_matrix = pd.DataFrame(dog_tf_words, columns=tvec.get_feature_names())\n",
    "\n",
    "dog_matrix.sum().sort_values(ascending=False).head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
